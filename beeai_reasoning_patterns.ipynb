{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDMNr1S86fWn"
      },
      "source": [
        "BeeAI Framework Reasoning Patterns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv4UxF3f3_-c"
      },
      "source": [
        "ðŸŽ¯ Scenario: The Field Marketing Lead has asked you to help prepare their team for conference season. You create a Conference Prep Agent that uses 3 tools: web search to collect relevant news and search for up to date information, Wikipedia to provide company history and details, and the team's internal notes and artifacts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CguLI81u3_gg"
      },
      "source": [
        "ðŸ“š What You'll Learn\n",
        "\n",
        "- Reasoning Patterns\n",
        "- Conditional Requirements - Enforcing business logic and rules\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk5sBse_66_H"
      },
      "source": [
        "## ðŸ”§ Setup\n",
        "First, let's install the BeeAI Framework and set up our environment.\n",
        "\n",
        "- setting up the observability so we can capture and log the actions our agent takes\n",
        "- getting the \"internal documents\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivhQKPrL652y"
      },
      "outputs": [],
      "source": [
        "%pip install -Uqq arize-phoenix s3fs unstructured \"requests==2.32.4\"\\\n",
        " \"openinference-instrumentation-beeai==0.1.13\" \\\n",
        " \"beeai-framework[duckduckgo,rag,wikipedia]\" \"fsspec==2025.3.0\" jedi \\\n",
        " \"opentelemetry-api==1.37.0\" \"opentelemetry-sdk==1.37.0\" \"langgraph<=0.5.0\" wikipedia\n",
        "\n",
        "# The following wraps Notebook output\n",
        "from IPython.display import HTML, display\n",
        "def set_css(*_, **__):\n",
        "    display(HTML(\"\\n<style>\\n pre{\\n white-space: pre-wrap;\\n}\\n</style>\\n\"))\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF_sHdJY7LnK"
      },
      "source": [
        "Now let's import the necessary modules:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN1XO5lj7MoB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "import time\n",
        "import phoenix as px\n",
        "import ipywidgets\n",
        "from typing import Any\n",
        "from datetime import date\n",
        "from pydantic import BaseModel, Field\n",
        "from dotenv import load_dotenv\n",
        "from beeai_framework.agents.requirement import RequirementAgent\n",
        "from beeai_framework.agents.requirement.types import RequirementAgentOutput\n",
        "from beeai_framework.agents.requirement.requirements import Requirement, Rule\n",
        "from beeai_framework.agents.requirement.requirements.conditional import ConditionalRequirement\n",
        "from beeai_framework.backend import ChatModel, ChatModelParameters\n",
        "from beeai_framework.backend.document_loader import DocumentLoader\n",
        "from beeai_framework.backend.embedding import EmbeddingModel\n",
        "from beeai_framework.backend.text_splitter import TextSplitter\n",
        "from beeai_framework.backend.vector_store import VectorStore\n",
        "from beeai_framework.context import RunContext\n",
        "from beeai_framework.emitter.emitter import Emitter, EventMeta\n",
        "from beeai_framework.emitter.types import EmitterOptions\n",
        "from beeai_framework.memory import UnconstrainedMemory\n",
        "from beeai_framework.middleware.trajectory import GlobalTrajectoryMiddleware\n",
        "from beeai_framework.tools import StringToolOutput, Tool, tool\n",
        "from beeai_framework.tools.search.duckduckgo import DuckDuckGoSearchTool\n",
        "from beeai_framework.tools.search.retrieval import VectorStoreSearchTool\n",
        "from beeai_framework.tools.search.wikipedia import WikipediaTool, WikipediaToolInput\n",
        "from beeai_framework.tools.think import ThinkTool\n",
        "from beeai_framework.tools.weather import OpenMeteoTool\n",
        "from beeai_framework.tools.types import ToolRunOptions\n",
        "from openinference.instrumentation.beeai import BeeAIInstrumentor\n",
        "from opentelemetry import trace as trace_api\n",
        "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
        "from opentelemetry.sdk import trace as trace_sdk\n",
        "from opentelemetry.sdk.resources import Resource\n",
        "from opentelemetry.sdk.trace.export import SimpleSpanProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lweJWWT0BJI1"
      },
      "source": [
        " ## 1ï¸âƒ£ LLM Providers: Choose Your AI Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9YkgZafBnFO"
      },
      "source": [
        "BeeAI Framework supports 10+ LLM providers including Ollama, Groq, OpenAI, Watsonx.ai, and more, giving you flexibility to choose local or hosted models based on your needs. In this workshop we'll be working Ollama, so you will be running the model locally. You can find the documentation on how to connect to other providers [here](https://framework.beeai.dev/modules/backend).\n",
        "\n",
        "In Colab, install and start Ollama for providing the Embedding Model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYKHEd3ZxhOz"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh > /dev/null\n",
        "!nohup ollama serve >/dev/null 2>&1 &\n",
        "\n",
        "from google.colab import userdata\n",
        "# Ollama - No parameters required\n",
        "provider=\"ollama\"\n",
        "model=\"granite4:micro\"\n",
        "#model=\"granite4:tiny-h\"\n",
        "#model=\"granite3.3\"\n",
        "provider_model=provider+\":\"+model\n",
        "!ollama pull $model\n",
        "llm=ChatModel.from_name(provider_model, ChatModelParameters(temperature=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NarMFy4272JG"
      },
      "source": [
        "## 2ï¸âƒ£ Provide a System Prompt, Memory Strategy, and Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FOCLkIn73sk"
      },
      "outputs": [],
      "source": [
        "todays_date = date.today().strftime(\"%B %d, %Y\")\n",
        "instruct_prompt = f\"\"\"You help field marketing teams prep for conferences by answering questions on companies that they need to prepare to talk to. You produce quick and actionable briefs, doing your best to anwer the user's question.\n",
        "\n",
        "Today's date is {todays_date}.\n",
        "\n",
        "Tools:\n",
        "- ThinkTool: Helps you plan and reason before you act. Use this tool when you need to think.\n",
        "- DuckDuckGoSearchTool: Use this tool to collect current information on agendas, speakers, news, competitor moves. Include title + date + link to the resources you find in your answer. Do not use this tool for internal notes or artifacts.\n",
        "- wikipedia_tool: Use this tool to get company/org history (not for breaking news). Only look up company names as the input.\n",
        "- internal_document_search: past meetings, playbooks, artifacts. If you use information from this in yoour response, label it as as [Internal]. Always use this tool when internal notes or content is references.\n",
        "\n",
        "Basic Rules:\n",
        "- Be concise and practical.\n",
        "- Favor recent info (agenda/news â‰¤30 days; exec changes/funding â‰¤180 days); flag older items.\n",
        "- If you don't know, say so. Don't make things up.\n",
        "\"\"\"\n",
        "\n",
        "memory = UnconstrainedMemory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1r2mzGnxf1Q"
      },
      "source": [
        "### Add Tools\n",
        "\n",
        "- The **Think tool** encourages a Re-Act pattern where the agent reasons and plans before calling a tool.\n",
        "\n",
        "- The **DuckDuckGoSearchTool** is a Web Search tool that provides relevant data from the internet to the LLM\n",
        "\n",
        "- We create a custom tool for searching Wikipedia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hA0-Yo8r17M"
      },
      "outputs": [],
      "source": [
        "think_tool = ThinkTool()\n",
        "\n",
        "search_tool = DuckDuckGoSearchTool()\n",
        "\n",
        "@tool\n",
        "async def wikipedia_tool(query: str) -> str:\n",
        "  \"\"\"\n",
        "  Search factual and historical information, including biography, history, politics, geography, society, culture,\n",
        "  science, technology, people, animal species, mathematics, and other subjects.\n",
        "\n",
        "  Args:\n",
        "      query: The topic or question to search for on Wikipedia.\n",
        "\n",
        "  Returns:\n",
        "      The information found via searching Wikipedia.\n",
        "  \"\"\"\n",
        "  full_text = False\n",
        "  language = \"en\"\n",
        "  tool = WikipediaTool(language=language)\n",
        "  response = await tool.run(WikipediaToolInput(query=query, full_text=False))\n",
        "  return response.get_text_content()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zwInfQoGhNc"
      },
      "source": [
        "### Add a RAGTool to Search Internal Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfzhF8B1cl0I"
      },
      "outputs": [],
      "source": [
        "# Pull in an embedding model into ollama\n",
        "!ollama pull nomic-embed-text:latest\n",
        "embedding_model = EmbeddingModel.from_name(\"ollama:nomic-embed-text\")\n",
        "\n",
        "# Read the synthetic data from the public github repo\n",
        "!curl --output rag_conference_prep_agent.txt https://raw.githubusercontent.com/IBM/beeai-workshop/refs/heads/main/intro_beeai_framework/rag_conference_prep_agent.txt\n",
        "with open('rag_conference_prep_agent.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "#Load the document using the DocumentLoader and split the document into chunks using the text_splitter.\n",
        "loader = DocumentLoader.from_name(\n",
        "    name=\"langchain:UnstructuredMarkdownLoader\", file_path=\"rag_conference_prep_agent.txt\"\n",
        ")\n",
        "try:\n",
        "    documents = await loader.load()\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load documents: {e}\")\n",
        "\n",
        "# Split documents into chunks\n",
        "text_splitter = TextSplitter.from_name(\n",
        "    name=\"langchain:RecursiveCharacterTextSplitter\", chunk_size=1000, chunk_overlap=200\n",
        ")\n",
        "try:\n",
        "    documents = await text_splitter.split_documents(documents)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to split documents: {e}\")\n",
        "print(f\"Loaded {len(documents)} document chunks\")\n",
        "\n",
        "# Create temporal vector store, which tracks time, and add documents\n",
        "vector_store = VectorStore.from_name(name=\"beeai:TemporalVectorStore\", embedding_model=embedding_model)\n",
        "await vector_store.add_documents(documents=documents)\n",
        "print(\"Vector store populated with documents\")\n",
        "\n",
        "# Create the vector store search tool. VectorStoreSearchTool is a built in tool wrapper\n",
        "internal_document_search = VectorStoreSearchTool(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkI0FLQfyrqv"
      },
      "source": [
        "## Conditional Requirements: Guiding Agent Behavior\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUVF44hCzDIE"
      },
      "source": [
        "What Are Conditional Requirements?\n",
        "[Conditional requirements](https://framework.beeai.dev/experimental/requirement-agent#conditional-requirement) ensure your agents are reliable by controlling when and how tools are used. They're like business rules for agent behavior. You can make them as strict (esentially writing a static workflow) or flexible (no rules! LLM decides) as you'd like.\n",
        "\n",
        "The rules that you enforce may seem simple in the BeeAI framework, but in other frameworks they require ~5X the amount of code. Check out this [blog](https://beeai.dev/blog/reliable-ai-agents) where we built the same agent in BeeAI and other agent framework LangGraph."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use widgets to choose reasoning patterns\n",
        "patterns=ipywidgets.ToggleButtons(\n",
        "    options=['Simple ReAct','Constrained ReAct','Planning','Unconstrained'],\n",
        "    value='Simple ReAct'\n",
        "    )\n",
        "display(patterns)"
      ],
      "metadata": {
        "id": "jtMR1S-YGVS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = patterns.value\n",
        "if pattern == 'Simple ReAct':\n",
        "  requirement_set=[ConditionalRequirement(ThinkTool, force_at_step=1, force_after=Tool,consecutive_allowed=False)]\n",
        "elif pattern == 'Constrained ReAct':\n",
        "  requirement_set=[\n",
        "    ConditionalRequirement(ThinkTool, consecutive_allowed=False, force_at_step=1 ),\n",
        "    ConditionalRequirement(wikipedia_tool, only_after=ThinkTool, consecutive_allowed=True, priority=10,),\n",
        "    ConditionalRequirement(DuckDuckGoSearchTool, only_after=ThinkTool, consecutive_allowed=True, min_invocations=1, max_invocations=3 ,priority=15,),\n",
        "    ConditionalRequirement(internal_document_search, only_after=ThinkTool, consecutive_allowed=True, min_invocations=1, priority=20,),\n",
        "  ]\n",
        "elif pattern == 'Planning':\n",
        "  requirement_set=[\n",
        "    ConditionalRequirement(ThinkTool, force_at_step=1 ),\n",
        "    ConditionalRequirement(wikipedia_tool, only_after=ThinkTool),\n",
        "    ConditionalRequirement(DuckDuckGoSearchTool, only_after=wikipedia_tool),\n",
        "    ConditionalRequirement(internal_document_search, only_after=DuckDuckGoSearchTool),\n",
        "  ]\n",
        "elif pattern == 'Unconstrained':\n",
        "  requirement_set=[]\n",
        "else:\n",
        "  requirement_set=[]"
      ],
      "metadata": {
        "id": "F5BE7hCiGVBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFNs-1wIKf9q"
      },
      "source": [
        "## Explore Observability: See what is happening under the hood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty1ivHWdgs5A"
      },
      "source": [
        "Create the function that sets up observability using `OpenTelemetry` and [Arize's Phoenix Platform](https://arize.com/docs/phoenix/inferences/how-to-inferences/manage-the-app). There a several ways to view what is happening under the hood of your agent. View the observability documentation [here](https://framework.beeai.dev/modules/observability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Quqo7VbDgph9"
      },
      "outputs": [],
      "source": [
        "def setup_observability(endpoint: str = \"http://localhost:6006/v1/traces\") -> None:\n",
        "    \"\"\"\n",
        "    Sets up OpenTelemetry with OTLP HTTP exporter and instruments the beeai framework.\n",
        "    \"\"\"\n",
        "    resource = Resource(attributes={})\n",
        "    tracer_provider = trace_sdk.TracerProvider(resource=resource)\n",
        "    tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n",
        "    trace_api.set_tracer_provider(tracer_provider)\n",
        "\n",
        "    BeeAIInstrumentor().instrument()\n",
        "\n",
        "load_dotenv()\n",
        "# Enable OpenTelemetry integration\n",
        "setup_observability(\"http://localhost:6006/v1/traces\")\n",
        "px_session = px.launch_app()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbktgviaGY-A"
      },
      "source": [
        "##  Assemble and Test your BeeAI Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q01Efmh0fuCv"
      },
      "source": [
        "This is the part we've been working towards! Let's assemble the agent with all the parts we created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8e74HSg_omm"
      },
      "outputs": [],
      "source": [
        "agent = RequirementAgent(\n",
        "    llm=llm,\n",
        "    instructions= instruct_prompt,\n",
        "    memory = memory,\n",
        "    tools=[ThinkTool(), DuckDuckGoSearchTool(), wikipedia_tool, internal_document_search],\n",
        "    requirements=requirement_set,\n",
        "    # Log intermediate steps to the console\n",
        "    middlewares=[GlobalTrajectoryMiddleware(included=[Tool])],\n",
        ")\n",
        "\n",
        "question = \"I'm planning on meeting the Moderna rep at the next conference. Give me a one pager and remind me where we left off on previous internal discussions.\"\n",
        "\n",
        "response = await agent.run(question, max_retries_per_step=3, total_max_retries=25)\n",
        "print(response.last_message.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMTy7_Q__tSN"
      },
      "outputs": [],
      "source": [
        "px_session.view()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}